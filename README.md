<center>

![Logo Sauter](https://raw.githubusercontent.com/cicerooficial/desafio-case-python-sauter/main/img/logo_sauter.png)

</center>

# Desafio Sauter: Capturar dados de Loja de Aplicativo - Google Play

<p align="center">
    <img src="https://img.shields.io/static/v1?label=STATUS&message=EM%20CONCLU√çDO&color=RED&style=for-the-badge"/>
    <img src="https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252"/>
    <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
    <img src="https://img.shields.io/badge/Pandas-2C2D72?style=for-the-badge&logo=pandas&logoColor=white"/>
	<img src="https://img.shields.io/badge/Google_Cloud-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white"/>
    <img src="https://img.shields.io/badge/Docker-2CA5E0?style=for-the-badge&logo=docker&logoColor=white"/>
	<img src="https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white"/>

</p>

- Autor: C√≠cero Henrique dos Santos
- E-mail: cicerooficial@gmail.com
- Linkedin: https://www.linkedin.com/in/cicero-henrique-santos/

------

## üìå T√≥picos:
- [Descri√ß√£o do projeto](https://github.com/cicerooficial/desafio-case-python-sauter#descri%C3%A7%C3%A3o-do-projeto)
- [Objetivos](https://github.com/cicerooficial/desafio-case-python-sauter#descri%C3%A7%C3%A3o-do-projeto)
- [Pr√©-requisitos](https://github.com/cicerooficial/desafio-case-python-sauter#descri%C3%A7%C3%A3o-do-projeto)
- [Como rodar a aplica√ß√£o](https://github.com/cicerooficial/desafio-case-python-sauter#descri%C3%A7%C3%A3o-do-projeto)
- [Tarefas em aberto](https://github.com/cicerooficial/desafio-case-python-sauter#descri%C3%A7%C3%A3o-do-projeto)
- [Linguagens, dependencias e libs utilizadas](https://github.com/cicerooficial/desafio-case-python-sauter#descri%C3%A7%C3%A3o-do-projeto)
- [Refer√™ncias](https://github.com/cicerooficial/desafio-case-python-sauter#descri%C3%A7%C3%A3o-do-projeto)
- [Conclus√£o](https://github.com/cicerooficial/desafio-case-python-sauter#descri%C3%A7%C3%A3o-do-projeto)

------

## üìù Descri√ß√£o do projeto
O Desafio Python tem como objetivo refletir alguns desafios que um Engenheiro de Dados possa enfrentar na [Sauter](https://sauter.digital/).

‚ö† √â importante saber que h√° m√∫ltiplos formatos para a resolu√ß√£o do desafio e ser√° necess√°rio consultar documenta√ß√µes (algumas das quais est√£o identificadas abaixo).

------

## üéØ Objetivos

### ‚úÖ Tarefa 1: 
Sua primeira tarefa √© utilizar a library google-play-scraper para capturar dados de apps.
O app selecionado √© o [Alexa](https://play.google.com/store/apps/details?id=com.amazon.dee.app), da Amazon.

1. Utilizando as informa√ß√µes de avalia√ß√£o do aplicativo, voc√™ deve chegar em um Data Frame de review parecido com o demonstrado abaixo:

![DataFrame_Example](https://raw.githubusercontent.com/cicerooficial/desafio-case-python-sauter/main/img/df_example.png)

A partir desse Data Frame, seguem as atividades propostas na tarefa:

2.  Criar 3 arquivos .csv a partir do dataframe, com a seguinte classifica√ß√£o:
    1. aval_positiva.csv para score maior ou igual a 4; 
    2. aval_neutra.csv para score igual a 3;
    3. aval_negativa.csv para score inferior a 3.

3.  Criar um report simples para essas vari√°veis utilizando a library [pandas-profiling](https://pandas-profiling.github.io/pandas-profiling/docs/master/index.html) para cada uma das separa√ß√µes (aval_neutra, aval_positiva, aval_negativa). </br>
‚ö† √â importante notar os principais pontos de cada an√°lise para sua apresenta√ß√£o.</br>
Finalmente, salvar o resultado do profile em formato .html.

### ‚úÖ Tarefa 2: 
1. A partir dos dados criados, subir as tabelas para um banco de dados.
 Aqui √© completamente opcional qual banco de dados utilizar, mas considerar utilizar o [BigQuery](https://cloud.google.com/bigquery/docs/tables) da Google, pois √© totalmente gratuito (para o tamanho do dataset) e em cloud.

    ‚ö† Caso prefira utilizar outro banco de dados de seu dom√≠nio tamb√©m vale como problema resolvido.

### ‚úÖ Tarefa 3: 
1. Criar objeto com opera√ß√µes de captura de dados, com atualiza√ß√£o da tabela. O objetivo aqui √© criar um pipeline simplificado de dados para o banco, de forma que a tabela seja sempre atualizada com as √∫ltimas informa√ß√µes de reviews.

#### Design de pipeline proposto com Airflow:    
![Design Pipeline Airflow](https://raw.githubusercontent.com/cicerooficial/desafio-case-python-sauter/main/img/design_pipeline_airflow.png)

------

## ‚ùó Pr√©-requisitos

### Tarefa 1
 Para realiza√ß√£o da Tarefa 1 utilize alguma plataforma notebook de sua prefer√™ncia.

Exemplo: 
[Google Colab](https://colab.research.google.com/?hl=pt-BR&skip_cache=false%22)
[Jupiter Notebook](https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/HEAD?urlpath=lab/tree/demo)

Entre outros...

### Tarefa 2
Para realiza√ß√£o da Tarefa 2 √© poss√≠vel realizar de forma manual pela plataforma do [Google CLoud Platform](https://cloud.google.com/free) ou atrav√©s de comandos no pr√≥prio Google Colab ou Jupiter Notebook. 

### Tarefa 3
Para a Tarefa 3, configure o docker em sua m√°quina (no caso utilizei Windows) seguindo os passos abaixo:

### Configurando o Docker em sua m√°quina:

#### Download Docker: [Docker](https://docs.docker.com/get-docker/)
#### Download Docker Compose: [Docker Compose](https://docs.docker.com/compose/install/)


#### Instala√ß√£o Docker - Windows

1. Verificar se o Windows est√° atualizado. Caso seja inferiro a 18362, clique no link ao lado para atualizar o Windows 10. [Atualizar o Windows](https://www.microsoft.com/pt-br/software-download/windows10);
2. Pesquise por Ativar ou desativar recursos do Windows e siga os passos abaixo:
    - Desativar Hyper-V;
    - Desativar Plataforma do Hipervisor do Windows;
    - Habilitar a Plataforma de M√°quina Virtual;
    - Habilitar o Subsistema do Windows para Linux (WSL).
3. Fa√ßa o Download do WSL 2 clicando no link ao lado: [Download WSL 2](https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi);
4. Acesse a Microsoft Store, e fa√ßa download e instale a distribui√ß√£o Linux Ubuntu 20.04 LTS (recomendado);
5. Instale o Docker Desktop no Windows: [Docker Desktop](https://hub.docker.com/editions/community/docker-ce-desktop-windows);
    - Obs: Abra o Docker Desktop e verifique se est√£o habilitados o "Enable integration with my default WSL distro" e "Ubuntu-20.04" em Settings->Resource->WSL Integration.

#### Instala√ß√£o do Apache Airflow no Docker
- Documenta√ß√£o - [Executando o Airflow no Docker](https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html)
- Abra o terminal do WSL2 e crie um diret√≥rio para o projeto no ***home/<seu-diret√≥rio>*** com o comando: `mkdir apache-airflow`;
- Acesse o diret√≥rio `cd apache-airflow`;
- No diret√≥rio apache-airflow, baixe o arquivo `docker-compose.yaml` disponibilizado na documenta√ß√£o oficial atrav√©s do comando: `curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml'`;
- Confirme se o arquivo foi baixado com o comando de listar: `ls`;
- No Linux , o in√≠cio r√°pido precisa saber a id do usu√°rio do host e precisa ter a id do grupo definida como 0. Caso contr√°rio, os arquivos criados no dags, logs e plugins ser√£o criados com usu√°rio root. Voc√™ deve certificar-se de configur√°-los para o docker-compose:
    `mkdir -p ./dags ./logs ./plugins` | `echo -e "AIRFLOW_UID=$(id -u)" > .env`;
- Em todos os sistemas operacionais, voc√™ precisa executar migra√ß√µes de banco de dados e criar a primeira conta de usu√°rio. Para fazer isso, execute:
`docker-compose up airflow-init`;
- Inicie todos os servi√ßos atrav√©s do comando: `docker-compose up -d`;
![Instala√ß√£o Airflow Terminal](https://raw.githubusercontent.com/cicerooficial/desafio-case-python-sauter/main/img/airflow1.png)
- Acesse o servidor web do Apache Airflow dispon√≠vel em: http://localhost:8080. A conta padr√£o possui o login **airflow** e a senha **airflow**.
![Acesso ao Apache Airflow](https://raw.githubusercontent.com/cicerooficial/desafio-case-python-sauter/main/img/airflow2.png)

------

## üîÑ Como rodar a aplica√ß√£o

- Tarefa 1 - Capturar dados de Loja de Aplicativo - Google Play
    1. Acessar o Google Colab: https://colab.research.google.com/drive/1ak9TAlvzWBj5Hh39swM8iG-uF1dVaDe6?usp=sharing;
    2. Acessar diretamente o arquivo no diret√≥rio: 
    [Google Colab - Capturando dados com Google-Play-Scraper](https://github.com/cicerooficial/desafio-case-python-sauter/blob/main/google_play_scraper_alexa.ipynb)
    - [Pasta com arquivos CSV](https://github.com/cicerooficial/desafio-case-python-sauter/tree/main/csv);
    - [Pasta com reports em html](https://github.com/cicerooficial/desafio-case-python-sauter/tree/main/reports).
    
- Tarefa 2 - Subir as tabelas para um BD
    1. Acesse o arquivo com o passo a passo manual: [Armazenar dados no BigQuery](https://github.com/cicerooficial/desafio-case-python-sauter/blob/main/Armazenar_dados_no_BigQuery.md);
    2. Siga os passos dentro do Google Colab [Google Colab - Capturando dados com Google-Play-Scraper](https://github.com/cicerooficial/desafio-case-python-sauter/blob/main/google_play_scraper_alexa.ipynb) no t√≥pico **Enviar dados CSV para bucket no Cloud Storage e Big Query**.

- Tarefa 3 - Criar Pipeline de dados
    1. Transfira os arquivos do pipeline da [Pasta dags](https://github.com/cicerooficial/desafio-case-python-sauter/tree/main/dags) para a pasta dags do Airflow (esta pasta foi criada no momento da instala√ß√£o juntamente com as pastas logs e plugins no √∫ltimo passo do t√≥pico **Instala√ß√£o do Apache Airflow no Docker**). Utilize o comando `cp /mnt/c/Users/<caminho onde at√© a pasta desafio-case-python-sauter>/dags /home/<seu diret√≥rio>/apache airflow/`;
    1. Acesso o arquivo etl_with_gcp_and_airflow.py dentro da pasta dags/etl_with_gcp_and_airflow. Dentro do arquivo, na fun√ßao `def __init__(self)` comente as linhas com o caminho para teste WINDOWS e descomente as linhas de teste para LINUX
    1. A fim de evitar conflitos por falta de depend√™ncias, execute o arquivo shell dentro da pasta ***\dags\etl_with_gcp_and_airflow***: 
        - Dentro da pasta mencionada, acesso o arquivo atrav√©s do de um editor de texto de sua orefer√™ncia (no caso utilizei o NANO). Atrav√©s do comando `nano pip_install_requirements.sh` desabilite os coment√°rios de acordo com seu sistema operacional. Salve com o comando `CTRL + O` e depois feche o editor de texto com o comando `CTRL + X`;
        ![Instala√ß√£o pip e requirements.txt](https://raw.githubusercontent.com/cicerooficial/desafio-case-python-sauter/main/img/instalacao_pip_requirements.png)
        - Agora (no WSL/Linux) execute o script atrav√©s do comando `sh pip_install_requirements.sh`.
    1. Crie uma conta se servi√ßo para autentica√ß√£o de acesso ao Google Cloud seguindo os passos atrav√©s do link: [Como criar uma conta de servi√ßo](https://cloud.google.com/docs/authentication/getting-started#creating_a_service_account) e salve o arquivo dentro da pasta \dags\etl_with_gcp_and_airflow;
    1. Com as configura√ß√µes necess√°rias realizadas, (caso n√£o esteja com o Apache Airflow iniciado) execute o comando `docker-compose up -d` dentro do diret√≥rio onde o arquivo se encontra: ***/home/<seu diret√≥rio>/apache airflow***;
    1. Acesse o servidor web do Apache Airflow dispon√≠vel em: http://localhost:8080. A conta padr√£o possui o login **airflow** e a senha **airflow**;
    1. Dentro do Apache Airflow, na aba DAGs, procure pelo DAG **pipeline_etl_with_gcp_and_airflow** e execute para certificar que tudo est√° funcionando corretamente. Por padr√£o as sinaliza√ß√µes de execu√ß√£o devem conter o status com a cor verde, indicando  **sucess** na execu√ß√£o.
        - Abaixo segue os prints de demonstra√ß√£o das Dags com status de Sucess: 
        - Pipeline completo no Airflow:
        ![Painel Pipeline OK 1 ](https://raw.githubusercontent.com/cicerooficial/desafio-case-python-sauter/main/img/pipeline_ok_2.png)
        ![Painel Pipeline OK 2](https://raw.githubusercontent.com/cicerooficial/desafio-case-python-sauter/main/img/pipeline_ok.png)        
    

------
## üìù Tarefas em aberto

‚¨ú 1. Criar exemplo de Deploy do pipeline via Airflow.

------
## üóÉ Linguagens, dependencias e libs utilizadas

|Lang/Lib/Framwork             |Version          |
|------------------------------|-----------------|
|Python                        |3.10             |
|google-play-scraper           |1.0.2            |
|pandas                        |1.3.5            |
|pandas-profiling              |3.1.0            |
|pandas_gbq                    |0.15.0           |
|Docker                        |4.3.2            |
|Docker Compose                |1.29.2           |
|Apache Airflow                |2.0.2            |
|google-cloud-storage          |1.43.0           |
|pip                           |21.3.1           |
|Google Cloud Storage (GCS)    |                 |
|Google BigQuery (GBQ)         |                 |
|Google Colab                  |                 |
|Visual Studio Code            |                 |

------
## üìö Refer√™ncias

- [Documenta√ß√£o google-play-scraper 1.0.2](https://pypi.org/project/google-play-scraper/)
- [Documenta√ß√£o Pandas API reference](https://pandas.pydata.org/docs/reference/)
- [Documenta√ß√£o Package pandas_profiling](https://pandas-profiling.github.io/pandas-profiling/docs/master/index.html)
- [Como criar um projeto no GCP](https://cloud.google.com/resource-manager/docs/creating-managing-projects?hl=pt-br&visit_id=637763797785493881-2801279509&rd=1#creating_a_project)
- [Como criar buckets de armazenamento](https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-gsutil)
- [Como criar um conjunto de dados](https://cloud.google.com/bigquery/docs/datasets#create-dataset)
- [Como carregar dados CSV em uma tabela BigQuery](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_table)
- [Executando o Airflow no Docker](https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html)
- [ETL com Airflow, Google Cloud Storage e BigQuery](https://github.com/okzapradhana/etl-flatfile-airflow#etl-with-airflow-google-cloud-storage-and-bigquery)
- [Google Cloud Client Libraries for google-cloud-storage](https://googleapis.dev/python/storage/latest/index.html)
- [Google Auth Library for Python](https://google-auth.readthedocs.io/en/master/reference/google.oauth2.service_account.html)
- [Documenta√ß√£o pandas-gbq](https://pandas-gbq.readthedocs.io/en/latest/)
- [Como fazer o download de objetos](https://cloud.google.com/storage/docs/downloading-objects#storage-download-object-python)
- [Como fazer upload de objetos](https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-code-sample)
- [Como configurar a vari√°vel de ambiente para autentica√ß√£o](https://cloud.google.com/docs/authentication/getting-started#windows)
- [Autentica√ß√£o com pandas-bq](https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html)
- [M√≥dulo de autentica√ß√£o google-auth](https://google-auth.readthedocs.io/en/master/reference/google.oauth2.service_account.html)
- [Documentation pip v21.3.1](https://pip.pypa.io/en/stable/user_guide/#requirements-files)
- [Documenta√ß√£o Apache Airflow Exemplo de Pipeline](https://airflow.apache.org/docs/apache-airflow/1.10.1/tutorial.html#example-pipeline-definition)
- [M√≥dulos de providers do google dispon√≠veis](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/index.html)

------

## üìé Conclus√£o
Com o desenvolvido do projeto foi poss√≠vel obter maiores conhecimentos de libs que facilitam o trabalho de ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) para o dia a dia de um Engenheiro de Dados, como:

1. Utilizar a lib google-plat-scraper em vez da comumente usada beautifulsoup; 
2. um diferencial foi conhecer a lib pandas-profiling, com acesso a fun√ß√µes de an√°lise de dados de forma r√°pida e bem completa para usos mais comuns, encurtando o processo de constru√ß√£o de scripts para cada tipo de an√°lise;
3. Utilizar o Google Coud Storage em formato de comandos, j√° que atrav√©s do qwiklabs √© demonstrado em formato manual em sua forma normal;
4. Um dos maiores desafios dentre os 3 foi buscar informa√ß√µes sobre como construir as DAGs no Apache Airflow. Sua documenta√ß√£o principal √© mais limitada, mas ap√≥s encontrar a documenta√ß√£o com [exemplos de Pipeline](https://airflow.apache.org/docs/apache-airflow/1.10.1/tutorial.html#example-pipeline-definition), foi poss√≠vel ter uma maior clareza e rapidez em montar as Tasks e DAGs. 

Por fim, o desafio foi uma √≥tima oportunidade de auto desafio, confian√ßa em desenvolver um projeto "real" utilizando tantas stacks diferentes e constru√ß√£o de um projeto que servir√° tamb√©m como portif√≥lio.
  
Aprendizados:

A maior dificuldade foi em tentar resolver o erro de *m√≥dulo google-play-scraper* n√£o identificado no Apache Airflow. 

Busquei desenvolver o pipeline completo seguindo o script etl_with_gcp_and_airflow.py, mas, devido o Apache Airflow ainda n√£o possuir dentro de seu reposit√≥rio de providers do Google suporte ao m√≥dulo do google-play-scrapper (conforme a documenta√ß√£o [M√≥dulos de providers do google dispon√≠veis](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/index.html)), n√£o foi poss√≠vel realizar o pipeline com a fun√ß√£o de capturar os dados, somente as fun√ß√µes de download dos CSVs do Cloud Storage e envio ao Big Query foi poss√≠vel realizar (Conforme solicitado no objetivo de tarefa 3).

Caso deseje executar o script completo, descomente as √∫ltimas linhas do arquivo da fun√ß√£o main, execute os comandos para evitar erros de depend√™ncias ao executar o script.