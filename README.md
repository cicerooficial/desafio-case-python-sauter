<center>
    <img src="data:image/png;base64,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" alt="Logo Sauter" style="zoom:100%;" />
</center>

# Desafio: Capturar dados de Loja de Aplicativo - Google Play

## Tópicos:
- [Descrição do projeto](#descrição-do-projeto)
- [Objetivos](#objetivos)
- [Deploy da Aplicação](#deploy-da-aplicação)
- [Pré-requisitos](#pré-requisitos)
- [Como rodar a aplicação](#como-rodar-a-aplicação)
- [Linguagens, dependencias e libs utilizadas](#linguagens-dependencias-e-libs-utilizadas)
- [Tarefas em aberto](#tarefas-em-aberto)

------

## Descrição do projeto
O Desafio Python tem como objetivo reflitir alguns desafios que um Engenheiro de Dados possa enfrentar na [Sauter](https://sauter.digital/).

⚠ É importante saber que há múltiplos formatos para a resolução do desafio e será necessário consultar documentações (algumas das quais colocaremos o link aqui).

------

## Objetivos

### Tarefa 1: 
Sua primeira tarefa é utilizar a library google-play-scraper para capturar dados de apps.
O app selecionado é o [Alexa](https://play.google.com/store/apps/details?id=com.amazon.dee.app), da Amazon.

1. ✅ Utilizando as informações de avaliação do aplicativo, você deve chegar em um Data Frame de review parecido com o demonstrado abaixo:

<img scr="img/df_example.png" alt="DataFrame_Example" /></br>

A partir desse Data Frame, seguem as atividades propostas na tarefa:

2.  ✅ Criar 3 arquivos .csv a partir do dataframe, com a seguinte classificação:
    1. aval_positiva.csv para score maior ou igual a 4; 
    2. aval_neutra.csv para score igual a 3;
    3. aval_negativa.csv para score inferior a 3.

3.  ✅ Criar um report simples para essas variáveis utilizando a library pandas profiling para
cada uma das separações (aval_neutra, aval_positiva, aval_negativa). </br>
⚠ É importante notar os principais pontos de cada análise para sua apresentação.</br>
Finalmente, salvar o resultado do profile em formato .html.

[Google Colab - Capturando dados com Google-Play-Scraper](https://github.com/cicerooficial/desafio-case-python-sauter/blob/main/google_play_scraper_alexa.ipynb)

### Tarefa 2: 
1. ✅ A partir dos dados criados, subir as tabelas para um banco de dados.
 Aqui é completamente opcional qual banco de dados utilizar, mas considerar utilizar o [BigQuery](https://cloud.google.com/bigquery/docs/tables) da Google, pois é totalmente gratuito (para o tamanho do dataset) e em cloud.

    ⚠ Caso prefira utilizar outro banco de dados de seu domínio também vale como problema resolvido.

### Tarefa 3: 
1. Criar objeto com operações de captura de dados, com atualização da tabela. O objetivo aqui é criar um pipeline simplificado de dados para o banco, de forma que a tabela seja sempre atualizada com as últimas informações de reviews.

------

## Pré-requisitos


------

## Como rodar a aplicação

Tarefa 1 - Capturar dados de Loja de Aplicativo - Google Play
1. Acessar o Google Colab: https://colab.research.google.com/drive/1ak9TAlvzWBj5Hh39swM8iG-uF1dVaDe6?usp=sharing;
2. Acessar o diretótio do arquivo baixado: [google-play-scraper-alexa.ipynb](https://github.com/cicerooficial/desafio-case-python-sauter/blob/main/google_play_scraper_alexa.ipynb)

Tarefa 2 - Subir as tabelas para um BD
1. Abrir o [Console Cloud Google](https://console.cloud.google.com/);
2. Criar um novo projeto;
3. Acessar Cloud Storage e criar um Bucket:
    1. No Console do Cloud, acesse a página [**Navegador do Cloud Storage**](https://console.cloud.google.com/storage/browser?_ga=2.166776635.46602346.1640698504-1290315837.1637543424&_gac=1.88127977.1638926009.Cj0KCQiAqbyNBhC2ARIsALDwAsBz4QTdJRXbw46J7nxFQENlWuf6ztbFKXQj4eT0UeZnUu1ddWMQpp0aAhTXEALw_wcB);
    2. Clique em **Criar bucket(Create Bucket)**;
    3. Na página **Criar um bucket(Create a Bucket)**, insira as informações do seu bucket. Para ir à próxima etapa, clique em Continuar:
        - Em Nomear o bucket, insira um nome que atenda aos requisitos de nome de bucket;
        - Em Escolha onde armazenar os dados, selecione o Tipo de local e o Local onde os dados do bucket serão armazenados permanentemente;
        - Em Escolha uma classe de armazenamento padrão para os dados, selecione uma classe de armazenamento para o bucket. A classe de armazenamento padrão é atribuída por padrão a todos os objetos carregados no bucket;
        - Em Escolha como controlar o acesso a objetos, selecione se o bucket aplica a prevenção de acesso público e um modelo de Controle de acesso para os objetos do bucket;
        - Em Escolha como proteger os dados do objeto, configure **Ferramentas de proteção** se quiser e selecione um método de **Criptografia de dados**.
    4. Clique em **Criar(Create)**;
    5. CLique sobre o nome do Bucket criado e selecione a opção **FAZER UPLOAD DE ARQUIVOS(UPLOAD FILES)** e selecione os arquivos que deseja enviar para o CLOUD STORAGE.
4. No Menu, no grupo BigData, selecione em BigQuery;
5. Clique sobre os 3 pontos do ID do projeto e selecione a opção **Criar conjunto de dados(Create Dataset)**:
    1. Defina o código do conjunto de dados e o local dos dados. Dê preferência para mesmo região onde o bucket foi instanciado;
    2. Clique em **CRIAR CONJUNTO DE DADOS(CREATE DATASET)**.
6. Ao lado do nome do Conjunto de dados(Dataset) criado, clique sobre os 3 pontos e selecione a opção **Criar Tabela(Create Table)**:
    1. Em origem, defina como Google Cloud Storage;
    2. Selecione o caminho do bucket, até o arquivo da tabela que deseja importar os dados; 
    3. Em Destino, defina um nome para o conjunto de dados(dataset) a ser criada no BigQuery;
    4. Também defina um nome para a tabela a ser criada no BigQuery;
    5. Esquema(Schema) marque a caixa de seleção como detectar automaticamente;
    6. Em opção avançada, em Delimitador de campo(Field delimiter) selecione Personalizado e no campo abaixo digite ; como delimitar.
    7. Clique em **Criar tabela(Create table)** para concluir.

Tarefa 3 - Criar Pipeline de dados

------

## Linguagens, dependencias e libs utilizadas

|Lang/Lib/Framwork    |Version          |
|---------------------|---------        |
|Python               |3.6, 3.7 ou 3.8  |
|Google-Play-Scraper  |v1.0.2           |
|Pandas               |v1.3.5           |
|Pandas Profiling     |v3.1.0           |



------
## Referências

- [google-play-scraper 1.0.2](https://pypi.org/project/google-play-scraper/)
- [Pandas API reference](https://pandas.pydata.org/docs/reference/)
- [Package pandas_profiling](https://pandas-profiling.github.io/pandas-profiling/docs/master/index.html)
- [Como criar buckets de armazenamento](https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-gsutil)
- 